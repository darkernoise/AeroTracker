'''
Created on Oct 20, 2016

@author: Joel Blackthorne

AeroTracker, Copyright (C) 2016 Joel Blackthorne
This file is part of AeroTracker.

AeroTracker is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

AeroTracker is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with AeroTracker.  If not, see <http://www.gnu.org/licenses/>.
'''
import collections
import time
import typing
import multiprocessing as mp
import numpy as np
from aero_tracker.trilateration.time_slice import TimeSlice
from aero_tracker.network.at_sensor_data_packet import AT_SensorDataPacket
from aero_tracker.trilateration.raw_data.ts_queue_item import TSQueueItem
from aero_tracker.trilateration.raw_data.ts_queue_item import TSQueueItemSensor
from aero_tracker.params.at_data_proc_server_params import AT_DataProcServerParams

class AT_TimeSliceQueueItem(object):
    '''
    This is the storage format for data packets while in the queue.
    '''
    time_slice = TimeSlice
    data_pkt = AT_SensorDataPacket
    
    def __lt__(self, other):
         return self.time_slice < other.time_slice
    
    def __init__(self, time_slice, data_pkt):
        self.time_slice = time_slice
        self.data_pkt = data_pkt
        return

class AT_TimeSliceQueue(object):
    '''
    Multiprocessing safe time slice data organization queue.
    '''
    MAX_QUEUE_SIZE = 1010
    MIN_ITEM_SIZE = 4 #This is the number of sensors needed to Trilaterate
    
    _buffer_size = 20 #set via param: TIME_SLICE_QUEUE_BUFFER
    _lock = mp.Lock()
    _triangulations_psec = 30
    _sensor_queues = None
    _params = AT_DataProcServerParams
    _buffer = []
    
    @property
    def get_queue(self):
        return self
    
    def enqueue(self, sensor_datpkt:AT_SensorDataPacket):
        '''
        Adds a data packet to the queue of the specific sensor.  Each sensor's 
        queue is stored in memory separately for speed. 
        '''
        time_slice = TimeSlice(time_val=sensor_datpkt.timestamp, sec_div=self._triangulations_psec)
        self._lock.acquire()
        delay_required = False
        
        buff_len = len(self._buffer)
        
        q_itm_snsr = TSQueueItemSensor(sensor_id=sensor_datpkt.sensor_id, data_pkt=sensor_datpkt)
        q_itm = TSQueueItem(tm_slc=time_slice, q_itm_snsr=q_itm_snsr)
        if (buff_len == 0):
            self._buffer.append(q_itm)
        else:
            try:
#                 indx = self._buffer.index(q_itm)
                indx = self._get_buffer_index(q_itm=q_itm, test_reverse=True, except_on_missing=True)
                #If we made it here, the value exits
                q_itm = self._buffer[indx]
                snsr_found = False
                for snsr in q_itm.sensors:
                    if (snsr.sensor_id == sensor_datpkt.sensor_id):
                        #sensor already exists
                        snsr.data_pkts.append(sensor_datpkt)
                        snsr_found = True
                        exit
                if (not snsr_found):
                    q_itm_snsr = TSQueueItemSensor(sensor_id=sensor_datpkt.sensor_id, data_pkt=sensor_datpkt)
                    q_itm.sensors.append(q_itm_snsr)
                
            except:
                #Value does not exist
                self._buffer.append(q_itm)
                self._buffer.sort()
            
        #check for overflow
        if (buff_len > self.MAX_QUEUE_SIZE):
            self._buffer.pop(0)
            delay_required = True
        elif(buff_len > self._params.TIME_SLICE_QUEUE_BUFFER):
            delay_required = True
        
        self._lock.release()
        
        if (delay_required):
            #impose a delay to slow this process down and to let others catch up
            time.sleep(0.01)
    
        return
    
    def pop(self):
        rval = None
#         tries = 0
#         while ((tries < 1) and (rval == None) and (len(self._buffer) > self._params.TIME_SLICE_QUEUE_BUFFER)):
        self._lock.acquire()
        rval = self._get_pop_value()
        self._lock.release()
#             tries += 1
        
#         if (rval != None):
#             print('Len: ' + str(len(rval.sensors)))
        return rval
    
    def _get_pop_value(self, seed_value=None, pop_count=0):
        rval = None
        
        buff_len = len(self._buffer)
        if (buff_len <= 0):
            return None
        
        new_val = None
        force_pop = False
        if (buff_len > pop_count):
            new_val = self._buffer[pop_count]
        
#       Combine seed value with new value
        if ((seed_value != None) and (new_val != None)):
            if (abs(new_val.sort_key - seed_value.sort_key) < self._params.MAX_TIME_SLICE_DIFF):
                #The two values are close enough that they will be added together
                
                new_val = TSQueueItem.merge_queue_items(q_old=seed_value, q_new=new_val)
                
            elif(buff_len > self._params.TIME_SLICE_QUEUE_BUFFER):
                #sort key values are too far apart, pop off the items used so far
                force_pop = True
        
        if (new_val != None):
            if ((len(new_val.sensors) >= self.MIN_ITEM_SIZE) or
                (buff_len >= self._params.TIME_SLICE_QUEUE_BUFFER) or
                (force_pop)):
                rval = new_val
                for i in range(0, pop_count + 1):
                    self._buffer.pop(0)
#             else:
#                 pop_count += 1
#                 rval = self._get_pop_value(seed_value=new_val, pop_count=pop_count)
        
        return rval
    
    def _get_buffer_index(self, q_itm:TSQueueItem, test_reverse=True, except_on_missing=True):
        rval = -1
        
        if (test_reverse):
            for i in range(len(self._buffer)-1,-1,-1):
                tst_q_itm = self._buffer[i]
                if (q_itm.sort_key == tst_q_itm.sort_key):
                    return i
                elif(q_itm.sort_key < tst_q_itm.sort_key):
                    #Not found in queue
                    if (except_on_missing):
                        raise Exception('Value not found')
                    return rval
        else: #forward
            for i in range(0, len(self._buffer)):
                tst_q_itm = self._buffer[i]
                if (q_itm.sort_key == tst_q_itm.sort_key):
                    return i
        
        if ((except_on_missing) and (rval < 0)):
            raise Exception('Value not found')
        
        return rval
        
    
#     def _get_pop_value_old(self):
#         '''
#         The pop activity is the first time that data from the various sensor queues is 
#         sorted and analyzed.
#         '''
#         rval = None
#         
#         pop_ts = self._get_pop_time_slice()
#         if (pop_ts != None):
#             for snsr_q_id in self._sensor_queues:
#                 snsr_q = self._sensor_queues[snsr_q_id]
#                 if (len(snsr_q) > 0):
#                     q_itm_snsr = None
#                     
#                     #Group all identical to the time slice or older
#                     snsr_added = False
#                     while ((len(snsr_q) > 0) and
#                            (snsr_q[0].time_slice.sort_key <= pop_ts)):
#                         snsr_added = True
#                         if (q_itm_snsr == None):
#                             q_itm_snsr = TSQueueItemSensor(sensor_id=snsr_q[0].data_pkt.sensor_id, data_pkt=snsr_q[0].data_pkt)
#                         else:
#                             q_itm_snsr.data_pkts.append(snsr_q[0].data_pkt)
#                         snsr_q.pop(0)
#                         
#                     #If there are no exact matches, check for an acceptable deviation       
#                     if (not snsr_added):
#                         if ((len(snsr_q) > 0) and
#                                ((snsr_q[0].time_slice.sort_key <= pop_ts) or 
#                                 (abs(snsr_q[0].time_slice.sort_key - pop_ts) <= self._params.MAX_TIME_SLICE_DIFF))):
#                             if (q_itm_snsr == None):
#                                 q_itm_snsr = TSQueueItemSensor(sensor_id=snsr_q[0].data_pkt.sensor_id, data_pkt=snsr_q[0].data_pkt)
#                             else:
#                                 q_itm_snsr.data_pkts.append(snsr_q[0].data_pkt)
#                             snsr_q.pop(0)
#                             
#                     #add the results grouped as a TSQueueItemSensor to the TSQueueItem
#                     if (q_itm_snsr != None):
#                         if (rval == None):
#                             rval = TSQueueItem(tm_slc=pop_ts, q_itm_snsr=q_itm_snsr)
#                         else:
#                             rval.sensors.append(q_itm_snsr)    
# 
#         return rval
    
    
    
#     def _get_pop_time_slice(self):
#         '''
#         Gets a timeslice from the queues to use for the pop.
#         '''
#         rval = None
#         
#         first_ts_list = []
#         do_pop = False
#         smallest_ts = 9999999999
#         queues_over_buffer_size = 0
#         
#         for snsr_q_id in self._sensor_queues:
#             snsr_q = self._sensor_queues[snsr_q_id]
#             if (len(snsr_q) > 0):
#                 first_ts_list.append(snsr_q[0].time_slice.sort_key)
#                 #Find the smallest time slice
#                 if (snsr_q[0].time_slice.sort_key < smallest_ts):
#                     smallest_ts = snsr_q[0].time_slice.sort_key
#                 #Count over buffer
#                 if (len(snsr_q) >= self._params.TIME_SLICE_QUEUE_BUFFER):
#                     queues_over_buffer_size += 1
#         
#         first_ts_list.sort()
#         
#         if (queues_over_buffer_size >= self.MIN_ITEM_SIZE):
#             do_pop = True
#         elif (len(self._sensor_queues) == len(first_ts_list)):
#             #Data is present is all queues
#             do_pop = True
#         elif (len(first_ts_list) > self.MIN_ITEM_SIZE):
#             std_dev = np.std(a=first_ts_list)
#             if (std_dev <= self._params.MAX_TIME_SLICE_DEVIATION):
#                 #Standard deviation between time stamp values is within acceptable limits
#                 do_pop = True
#         
#         if (do_pop):
#             median_ts = np.median(a=first_ts_list)
#             if (np.abs(median_ts - smallest_ts) < self._params.MAX_TIME_SLICE_DIFF):
#                 rval = median_ts
#             else:
#                 rval = smallest_ts
#         
#         return rval
#     
#     def _get_sensor_queue(self, sensor_id:str)->typing.List:
#         '''
#         Gets the queue for a sensor id.
#         '''
#         #ensure that the queue is created
#         if (self._sensor_queues == None):
#             #first one, so add
#             self._sensor_queues = {sensor_id:[]}
#         elif(not sensor_id in self._sensor_queues):
#             self._sensor_queues.update({sensor_id:[]})
#             
#         return self._sensor_queues[sensor_id]

    def __init__(self, params:AT_DataProcServerParams):
        '''
        Constructor
        '''
        self._lock = mp.Lock()
        self._params = params
        self._triangulations_psec = self._params.TRIANGULATIONS_PER_SECOND
        print('Trilaterations per/sec: ' + str(self._triangulations_psec))
        self._buffer = []
        self._buffer_size = self._params.TIME_SLICE_QUEUE_BUFFER
        if (self._buffer_size > self.MAX_QUEUE_SIZE):
            self._buffer_size = self.MAX_QUEUE_SIZE - 10
        return
    
    
#Tester
# import time
# params = AT_DataProcServerParams('/etc/popt/aero_tracker/at_data_proc_server.conf')
# obj = AT_TimeSliceQueue(params=params)        
# cluster_id = 'NP'
# target_id = 'target1234'
#    
# pkt_a = AT_SensorDataPacket(cluster_id=cluster_id, sensor_id=cluster_id + '_A', target_id=target_id, filter_dist=12.6, raw_dist=13.2, rssi=-34, timestamp=time.time() - 0.1)
# obj.enqueue(pkt_a)
# pkt_a = AT_SensorDataPacket(cluster_id=cluster_id, sensor_id=cluster_id + '_A', target_id=target_id, filter_dist=12.6, raw_dist=13.2, rssi=-34, timestamp=time.time() - 0.1)
# obj.enqueue(pkt_a)
#  
# pkt_base = AT_SensorDataPacket(cluster_id=cluster_id, sensor_id=cluster_id + '_BASE', target_id=target_id, filter_dist=12.6, raw_dist=13.2, rssi=-34, timestamp=time.time())
# obj.enqueue(pkt_base)
# pkt_b = AT_SensorDataPacket(cluster_id=cluster_id, sensor_id=cluster_id + '_B', target_id=target_id, filter_dist=12.6, raw_dist=13.2, rssi=-34, timestamp=time.time())
# obj.enqueue(pkt_b)
# pkt_c = AT_SensorDataPacket(cluster_id=cluster_id, sensor_id=cluster_id + '_C', target_id=target_id, filter_dist=12.6, raw_dist=13.2, rssi=-34, timestamp=time.time() + 0.2)
# obj.enqueue(pkt_c)
# pkt_d = AT_SensorDataPacket(cluster_id=cluster_id, sensor_id=cluster_id + '_D', target_id=target_id, filter_dist=12.6, raw_dist=13.2, rssi=-34, timestamp=time.time() + 0.1)
# obj.enqueue(pkt_d)
# pkt_a = AT_SensorDataPacket(cluster_id=cluster_id, sensor_id=cluster_id + '_A', target_id=target_id, filter_dist=12.6, raw_dist=13.2, rssi=-34, timestamp=time.time() - 0.1)
# obj.enqueue(pkt_a)
#    
# val = obj.pop()
# print(val)

        